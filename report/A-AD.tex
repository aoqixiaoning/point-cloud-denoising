\chapter{Automatic Differentiation}

The automatic differentiation is a technique used for computing the derivatives,
gradient of expressions. It is not the same as the symbolic or the numerical
differentiation.

Indeed, automatic differentiation can be used to compute derivatives of programs
and not only mathematical functions without using approximations like with
numerical differentiation.

Let us suppose that we want to compute the derivative of a function $ f $ with a
single one dimensional argument $ x $. To do that, we will replace the number
type that is to say that we will replace $ x $ by $ x + \epsilon x' $ where $
\epsilon $ with the property $ \epsilon^2 = 0 $. Then, we overload all the
arithmetical operations: addition, subtraction, multiplication, division.

Then we call $ f $ with this variable: $ f(x + \epsilon x') = y + \epsilon y'
$. We conclude that $ y = f(x) $ and $ y' = x' f'(x) $ . Indeed for small values
of $ \epsilon $, we have the first order Taylor expansion: $ f(x + \epsilon x') =
f(x) + \epsilon x' f'(x) + ... $. $ x' $ is called a seed and can be chosen
arbitrarily, we can for instance choose $ x ' = 1 $ and so $ y' = f'(x) $.

This process can be easily extended to handle functions like $ f : \R^n
\rightarrow \R $ in order to compute gradients of such functions. This is what
we did: we considered a function over $ n $ points as $ f : \R^{2n} \rightarrow
\R $.

This technique is interesting because it allows us to compute accurate
derivatives of functions very easily and efficiently. Indeed, we only have to
change the number type and write the good implementations of the basic
arithmetic operations.
In \texttt{CGAL}, this is easy to do since the library is already parametrized
by the number type.

% TODO

% vim: set spelllang=en :
